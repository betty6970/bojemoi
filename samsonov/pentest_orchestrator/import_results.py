#!/usr/bin/env python3
"""
Import existing result files from results/ directory into Faraday.
Supports batch import, retry of failed imports, and tracking of imported files.
"""

import os
import sys
import json
import argparse
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional

# Add plugins to path
script_dir = Path(__file__).parent
sys.path.insert(0, str(script_dir / "plugins"))

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

RESULTS_DIR = script_dir / "results"
IMPORTED_TRACKER = script_dir / "results" / ".imported"


def load_imported_tracker() -> Dict:
    """Load tracker of already imported files."""
    if IMPORTED_TRACKER.exists():
        try:
            with open(IMPORTED_TRACKER) as f:
                return json.load(f)
        except Exception:
            return {"imported": [], "failed": []}
    return {"imported": [], "failed": []}


def save_imported_tracker(tracker: Dict):
    """Save tracker of imported files."""
    IMPORTED_TRACKER.parent.mkdir(exist_ok=True)
    with open(IMPORTED_TRACKER, 'w') as f:
        json.dump(tracker, f, indent=2)


def get_result_files(include_imported: bool = False) -> List[Path]:
    """Get list of result files to import."""
    if not RESULTS_DIR.exists():
        logger.warning(f"Results directory not found: {RESULTS_DIR}")
        return []

    tracker = load_imported_tracker()
    imported = set(tracker.get("imported", []))

    files = []
    for f in RESULTS_DIR.glob("*.json"):
        if f.name.startswith("."):
            continue
        if not include_imported and f.name in imported:
            continue
        files.append(f)

    return sorted(files, key=lambda x: x.stat().st_mtime)


def import_file(filepath: Path, workspace: str = None, dry_run: bool = False) -> Dict:
    """Import a single result file into Faraday."""
    try:
        with open(filepath) as f:
            results = json.load(f)
    except json.JSONDecodeError as e:
        return {"status": "error", "error": f"Invalid JSON: {e}", "file": filepath.name}
    except Exception as e:
        return {"status": "error", "error": str(e), "file": filepath.name}

    # Use workspace from file or override
    ws = workspace or results.get("workspace", "default")

    if dry_run:
        scan_count = len(results.get("scans", []))
        return {
            "status": "dry_run",
            "file": filepath.name,
            "workspace": ws,
            "target": results.get("target", "unknown"),
            "scans": scan_count
        }

    # Import using Faraday plugin
    try:
        from plugin_faraday import import_results
        result = import_results(workspace=ws, results=results)
        result["file"] = filepath.name
        return result
    except ImportError as e:
        return {"status": "error", "error": f"Cannot load Faraday plugin: {e}", "file": filepath.name}
    except Exception as e:
        return {"status": "error", "error": str(e), "file": filepath.name}


def import_all(workspace: str = None, dry_run: bool = False,
               include_imported: bool = False, retry_failed: bool = False) -> Dict:
    """Import all result files into Faraday."""
    tracker = load_imported_tracker()
    files = get_result_files(include_imported=include_imported)

    # Add failed files if retrying
    if retry_failed:
        failed_files = [RESULTS_DIR / f for f in tracker.get("failed", []) if (RESULTS_DIR / f).exists()]
        files = list(set(files + failed_files))

    if not files:
        logger.info("No result files to import")
        return {"status": "no_files", "imported": 0, "failed": 0}

    logger.info(f"Found {len(files)} result file(s) to import")

    imported = []
    failed = []

    for filepath in files:
        logger.info(f"Processing: {filepath.name}")
        result = import_file(filepath, workspace=workspace, dry_run=dry_run)

        if result.get("status") in ("completed", "dry_run"):
            imported.append(filepath.name)
            logger.info(f"  ✓ {result.get('status')}: {result.get('imported', 0)} vulns imported to {result.get('workspace')}")
        else:
            failed.append(filepath.name)
            logger.error(f"  ✗ Failed: {result.get('error')}")

    # Update tracker (unless dry run)
    if not dry_run:
        tracker["imported"] = list(set(tracker.get("imported", []) + imported))
        tracker["failed"] = [f for f in failed if f not in tracker["imported"]]
        tracker["last_import"] = datetime.now().isoformat()
        save_imported_tracker(tracker)

    return {
        "status": "completed",
        "imported": len(imported),
        "failed": len(failed),
        "imported_files": imported,
        "failed_files": failed
    }


def list_files(show_all: bool = False):
    """List result files and their import status."""
    tracker = load_imported_tracker()
    imported = set(tracker.get("imported", []))
    failed = set(tracker.get("failed", []))

    if not RESULTS_DIR.exists():
        print(f"Results directory not found: {RESULTS_DIR}")
        return

    files = list(RESULTS_DIR.glob("*.json"))
    if not files:
        print("No result files found")
        return

    print(f"\nResult files in {RESULTS_DIR}:\n")
    print(f"{'Status':<12} {'File':<40} {'Size':<10} {'Modified'}")
    print("-" * 80)

    for f in sorted(files, key=lambda x: x.stat().st_mtime, reverse=True):
        if f.name.startswith("."):
            continue

        if f.name in imported:
            status = "✓ imported"
            if not show_all:
                continue
        elif f.name in failed:
            status = "✗ failed"
        else:
            status = "○ pending"

        size = f.stat().st_size
        mtime = datetime.fromtimestamp(f.stat().st_mtime).strftime("%Y-%m-%d %H:%M")
        size_str = f"{size / 1024:.1f}KB" if size > 1024 else f"{size}B"

        print(f"{status:<12} {f.name:<40} {size_str:<10} {mtime}")

    print()
    print(f"Total: {len(files)} files, {len(imported)} imported, {len(failed)} failed")


def reset_tracker():
    """Reset the import tracker."""
    if IMPORTED_TRACKER.exists():
        IMPORTED_TRACKER.unlink()
        print("Import tracker reset")
    else:
        print("No tracker to reset")


def main():
    parser = argparse.ArgumentParser(
        description='Import result files from results/ into Faraday'
    )
    parser.add_argument('-w', '--workspace',
                        help='Override workspace for all imports')
    parser.add_argument('-f', '--file',
                        help='Import a specific file')
    parser.add_argument('--dry-run', action='store_true',
                        help='Show what would be imported without actually importing')
    parser.add_argument('--include-imported', action='store_true',
                        help='Re-import already imported files')
    parser.add_argument('--retry-failed', action='store_true',
                        help='Retry previously failed imports')
    parser.add_argument('--list', action='store_true',
                        help='List result files and their status')
    parser.add_argument('--list-all', action='store_true',
                        help='List all result files including imported')
    parser.add_argument('--reset', action='store_true',
                        help='Reset import tracker')

    args = parser.parse_args()

    os.chdir(script_dir)

    if args.reset:
        reset_tracker()
        return

    if args.list or args.list_all:
        list_files(show_all=args.list_all)
        return

    if args.file:
        filepath = Path(args.file)
        if not filepath.exists():
            filepath = RESULTS_DIR / args.file
        if not filepath.exists():
            print(f"File not found: {args.file}")
            sys.exit(1)

        result = import_file(filepath, workspace=args.workspace, dry_run=args.dry_run)
        print(json.dumps(result, indent=2))

        if not args.dry_run and result.get("status") == "completed":
            tracker = load_imported_tracker()
            tracker["imported"] = list(set(tracker.get("imported", []) + [filepath.name]))
            save_imported_tracker(tracker)
    else:
        result = import_all(
            workspace=args.workspace,
            dry_run=args.dry_run,
            include_imported=args.include_imported,
            retry_failed=args.retry_failed
        )
        print(f"\nImport complete: {result['imported']} imported, {result['failed']} failed")


if __name__ == "__main__":
    main()
